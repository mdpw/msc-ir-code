{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "86da4c47-6cfd-4d36-88dc-129eee0b67f2",
      "metadata": {
        "id": "86da4c47-6cfd-4d36-88dc-129eee0b67f2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import subprocess\n",
        "import time\n",
        "from typing import List, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bfc1891e-a1e1-4a99-8e06-281a1bb4e4e9",
      "metadata": {
        "id": "bfc1891e-a1e1-4a99-8e06-281a1bb4e4e9"
      },
      "outputs": [],
      "source": [
        "class Particle:\n",
        "    def __init__(self, prompt_template: str, dim: int = 1):\n",
        "        # Each particle's \"position\" is its current text prompt (string)\n",
        "        self.position = np.array([prompt_template], dtype=object)\n",
        "        #how much and in what way the prompt should change. Currently it`s defined as zero\n",
        "        self.velocity = np.zeros(dim, dtype=float)  # Placeholder (not really used for text)\n",
        "        # best version of the prompt so far\n",
        "        self.best_position = self.position.copy()\n",
        "        # assign a negative value at the initialization point inside the constructor.\n",
        "        # hence, even a small imporvement becomes noticable\n",
        "        self.best_score = -np.inf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a2fac812-e30c-4306-9469-f68843b2b799",
      "metadata": {
        "id": "a2fac812-e30c-4306-9469-f68843b2b799"
      },
      "outputs": [],
      "source": [
        "def call_ollama(prompt: str, context_text: str, timeout: int = 60) -> str:\n",
        "    \"\"\"\n",
        "    Calls Ollama using subprocess. Requires `ollama serve` running in background.\n",
        "    \"\"\"\n",
        "    full_prompt = f\"{prompt}\\n\\nContext:\\n{context_text}\"\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"ollama\", \"run\", \"phi3:mini\", full_prompt],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                encoding=\"utf-8\",   # ðŸ‘ˆ force UTF-8 decoding\n",
        "                errors=\"replace\",   # ðŸ‘ˆ replace problematic characters instead of crashing\n",
        "                timeout=timeout\n",
        ")\n",
        "        return result.stdout.strip()\n",
        "    except subprocess.TimeoutExpired:\n",
        "        return \"[ERROR: Ollama call timed out]\"\n",
        "    except FileNotFoundError:\n",
        "        return \"[ERROR: Ollama binary not found in PATH]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "885d0c41-9267-4dd0-83e2-a26512e35f0f",
      "metadata": {
        "id": "885d0c41-9267-4dd0-83e2-a26512e35f0f"
      },
      "outputs": [],
      "source": [
        "def fitness_function(prompt: str, context_text: str) -> Tuple[float, str]:\n",
        "    \"\"\"Evaluate how good the output is for a given prompt + context.\"\"\"\n",
        "    output = call_ollama(prompt, context_text)\n",
        "    #Sometimes things go wrong (network hiccup, invalid prompt, etc.). If the AIâ€™s reply starts\n",
        "    # with [ERROR: ...], we say:\"Okay, thatâ€™s a flop. Give it the lowest possible score (0.0),\n",
        "    #and pass the error back.\n",
        "    if output.startswith(\"[ERROR:\"):  # error fallback\n",
        "        return 0.0, output\n",
        "\n",
        "\n",
        "    #Count how many words the output has. Why? Longer isnâ€™t always better, but a one-word answer\n",
        "    #like â€œYesâ€ is probably less useful.So this is basically: \"More words = at least some effort.\n",
        "    length_score = len(output.split())\n",
        "\n",
        "    #Now check for important keywords inside the AIâ€™s response. For example:\n",
        "    #if it includes â€œFAQ,â€ â€œquestion,â€ â€œanswer,â€ or â€œsummary.â€ Each time one appears, we count it.\n",
        "    # Why? Because those are the words we wanted the AI to emphasize in its answers.\n",
        "\n",
        "    keyword_hits = sum([kw in output.lower() for kw in [\"faq\", \"question\", \"answer\", \"summary\"]])\n",
        "    # custom score formulae.\n",
        "    # Every word adds 0.1 points. (so a 50-word response = 5 points).\n",
        "    # Every keyword hit adds 5 points. (so if it used \"summary\" and \"answer\" = +10 points).\n",
        "    score = length_score * 0.1 + keyword_hits * 5\n",
        "    return score, output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8e896165-a20e-463a-b336-73a3631bf95d",
      "metadata": {
        "id": "8e896165-a20e-463a-b336-73a3631bf95d"
      },
      "outputs": [],
      "source": [
        "def pso_optimize(prompts: List[str], context_text: str, num_iterations: int = 3):\n",
        "    particles = [Particle(prompt) for prompt in prompts]\n",
        "    global_best_score = -np.inf\n",
        "    global_best_prompt = None\n",
        "    global_best_output = \"\"\n",
        "\n",
        "    for it in range(num_iterations):\n",
        "        print(f\"\\n=== Iteration {it+1}/{num_iterations} ===\")\n",
        "\n",
        "        for idx, p in enumerate(particles):\n",
        "            # Evaluate\n",
        "            score, output = fitness_function(p.position[0], context_text)\n",
        "            print(f\"  Particle {idx+1}: Score={score:.2f}\")\n",
        "\n",
        "            # Update personal best\n",
        "            if score > p.best_score:\n",
        "                p.best_score = score\n",
        "                p.best_position = p.position.copy()\n",
        "\n",
        "            # Update global best\n",
        "            if score > global_best_score:\n",
        "                global_best_score = score\n",
        "                global_best_prompt = p.position[0]\n",
        "                global_best_output = output\n",
        "\n",
        "            # Mutate: tweak phrasing to explore search space\n",
        "            if random.random() < 0.3:\n",
        "                mutated = (\n",
        "                    p.position[0]\n",
        "                    .replace(\"Summarize\", \"Please summarize\")\n",
        "                    .replace(\"FAQ\", \"Frequently Asked Questions\")\n",
        "                    .replace(\"Q&A\", \"question and answer pairs\")\n",
        "                )\n",
        "                p.position[0] = mutated\n",
        "\n",
        "    return global_best_prompt, global_best_output, global_best_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "52700bf4-5dc1-4474-a793-eaa7d9d6e36c",
      "metadata": {
        "id": "52700bf4-5dc1-4474-a793-eaa7d9d6e36c"
      },
      "outputs": [],
      "source": [
        "context_text = \"\"\"\n",
        "Large Language Models (LLMs) are revolutionizing natural language processing.\n",
        "They can summarize documents, answer questions, and generate creative content.\n",
        "However, results vary drastically depending on how prompts are phrased.\n",
        "Finding the best formulation is known as prompt engineering.\n",
        "\"\"\"\n",
        "\n",
        "initial_prompts = [\n",
        "    \"Summarize the following document into an FAQ style with answers:\",\n",
        "    \"Create a Frequently Asked Questions section from this text:\",\n",
        "    \"Turn this document into 5 question-and-answer pairs, concise and clear:\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b1ffd6b7-88f4-4f4a-bc72-cb401e3dc75e",
      "metadata": {
        "id": "b1ffd6b7-88f4-4f4a-bc72-cb401e3dc75e",
        "outputId": "d27a9e5c-0f90-44c6-a73e-45522ce34e75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Iteration 1/2 ===\n",
            "  Particle 1: Score=0.00\n",
            "  Particle 2: Score=0.00\n",
            "  Particle 3: Score=0.00\n",
            "\n",
            "=== Iteration 2/2 ===\n",
            "  Particle 1: Score=0.00\n",
            "  Particle 2: Score=0.00\n",
            "  Particle 3: Score=0.00\n"
          ]
        }
      ],
      "source": [
        "best_prompt, best_output, best_score = pso_optimize(\n",
        "    initial_prompts,\n",
        "    context_text,\n",
        "    num_iterations=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d58df39d-76f0-4ffb-8c9d-cd56361a5a2b",
      "metadata": {
        "id": "d58df39d-76f0-4ffb-8c9d-cd56361a5a2b",
        "outputId": "336e0787-1942-44bc-a9a6-4e90815d1c9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ðŸ¥‡ Best Prompt Found ===\n",
            "Summarize the following document into an FAQ style with answers:\n",
            "\n",
            "=== ðŸ“˜ Sample Output (truncated) ===\n",
            "[ERROR: Ollama binary not found in PATH]\n",
            "\n",
            "Fitness Score: 0.0\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== ðŸ¥‡ Best Prompt Found ===\")\n",
        "print(best_prompt)\n",
        "\n",
        "print(\"\\n=== ðŸ“˜ Sample Output (truncated) ===\")\n",
        "print(best_output[:600])\n",
        "\n",
        "print(\"\\nFitness Score:\", best_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4cf56f56-b152-4675-8682-834330be43db",
      "metadata": {
        "id": "4cf56f56-b152-4675-8682-834330be43db"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "KDU",
      "language": "python",
      "name": "kdu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}